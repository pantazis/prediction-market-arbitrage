Here is a **rewritten, VS-Code-safe MASTER PROMPT**, explicitly **step-based, chunked, and crash-resistant**.
It is designed so Copilot / Codex / ChatGPT **never tries to analyze the whole repo at once**.

You can paste this **as-is** into VS Code instructions or into an AI tool.

---

PROMPT START
You are an expert Python reverse-engineer and codebase cartographer.

⚠️ HARD CONSTRAINTS (ANTI-CRASH RULES)

* NEVER analyze the entire repository in one pass.
* Work strictly in STEPS.
* Each step must process a LIMITED SCOPE (few files or one layer).
* After each step, OUTPUT JSON ONLY and STOP.
* The next step must explicitly state it consumes the previous step’s output.
* If information is missing, infer best-effort and mark `"uncertain": true`.
* NO markdown. NO explanations. JSON only.

GOAL
Produce a COMPLETE CODE MAP SCHEMA of the project for AI ingestion.

---

## STEP 0 — FILE INVENTORY ONLY (NO DEEP PARSING)

TASK:

* Read directory tree.
* Exclude: venv/, .venv/, build/, dist/, **pycache**/.
* Do NOT inspect file contents yet.

OUTPUT (JSON ONLY):
{
"step": 0,
"files": [
{
"path": "relative/path.py",
"type": "python|config|data|script|other"
}
]
}

STOP after output.

---

## STEP 1 — MODULE & ENTRY POINT DISCOVERY

(Consumes STEP 0 output)

TASK:

* For each `.py` file from STEP 0:

  * Infer module name.
  * Detect entry points:

    * `if __name__ == "__main__"`
    * CLI (`argparse`, `click`, `typer`)
    * Docker / script hints
* Detect runtime roles at HIGH LEVEL only.

OUTPUT:
{
"step": 1,
"project": {
"name": "<infer or unknown>",
"root": "<root path>",
"language": "python",
"python_version": "<infer or null>",
"entry_points": [
{
"type": "cli|script|module|docker|other",
"path": "path/to/file.py",
"call": "**main**|module:function",
"description": "short technical purpose"
}
],
"runtime_roles": {
"scanner": [],
"detectors": [],
"execution": [],
"reporting": [],
"telegram": [],
"api_clients": []
}
}
}

STOP.

---

## STEP 2 — IMPORT GRAPH (STATIC ONLY)

(Consumes STEP 1 output)

TASK:

* Parse imports only.
* No function bodies.
* Build module dependency graph.

OUTPUT:
{
"step": 2,
"files": [
{
"path": "file.py",
"module": "pkg.mod",
"imports": {
"stdlib": [],
"third_party": [],
"internal": []
}
}
],
"graphs": {
"module_dependency_edges": [
{ "from": "pkg.a", "to": "pkg.b", "type": "import" }
]
}
}

STOP.

---

## STEP 3 — SYMBOL EXTRACTION (SHALLOW)

(Consumes STEP 2 output)

TASK:

* Extract:

  * classes
  * functions
  * dataclasses
  * pydantic models
  * constants
* NO call graph yet.
* One file at a time.

OUTPUT:
{
"step": 3,
"symbols": [
{
"id": "pkg.mod:symbol",
"kind": "class|function|dataclass|pydantic_model|constant",
"file": "path.py",
"line_start": 0,
"line_end": 0,
"signature": "def x(...)",
"role": "scanner|detector|util|client|other"
}
]
}

STOP.

---

## STEP 4 — CALL GRAPH (BEST-EFFORT STATIC)

(Consumes STEP 3 output)

TASK:

* Infer calls via:

  * direct function calls
  * method calls
  * obvious pipelines
* Mark uncertainty where needed.

OUTPUT:
{
"step": 4,
"graphs": {
"call_edges": [
{
"from": "pkg.a:func",
"to": "pkg.b:func",
"type": "call",
"count_hint": "static",
"uncertain": true
}
]
}
}

STOP.

---

## STEP 5 — DATA MODELS & CONFIG FLOW

(Consumes STEP 4 output)

TASK:

* Identify config sources:

  * env vars
  * yaml/json
  * CLI args
* Identify data models and who consumes them.

OUTPUT:
{
"step": 5,
"contracts": {
"configs": [
{
"name": "AppConfig",
"source": "env|yaml|json|cli",
"fields": [],
"used_by": []
}
],
"interfaces": []
},
"graphs": {
"data_flow_edges": []
}
}

STOP.

---

## STEP 6 — SIDE EFFECTS & I/O

(Consumes STEP 5 output)

TASK:

* Detect filesystem, network, subprocess, telegram, DB usage.

OUTPUT:
{
"step": 6,
"io_edges": [],
"side_effects_summary": []
}

STOP.

---

## STEP 7 — FINAL MERGE

(Consumes ALL previous steps)

TASK:

* Merge all steps into ONE FINAL JSON matching the FULL SCHEMA.
* Add ai_hints and optional search_index.
* No new analysis.

OUTPUT:
{ FULL FINAL SCHEMA JSON }

STOP.

---

PROMPT END

---
